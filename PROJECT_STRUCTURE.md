# AceBuddy RAG Chatbot - Project Structure

```
acebuddy-rag-chatbot/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # Main documentation
â”œâ”€â”€ ğŸ“„ IMPLEMENTATION_PLAN.md             # Detailed implementation guide
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md               # This file
â”œâ”€â”€ ğŸ“„ requirements.txt                   # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example                       # Environment template
â”œâ”€â”€ ğŸ“„ .env                               # Your config (create this)
â”œâ”€â”€ ğŸ“„ .gitignore                         # Git ignore rules
â”œâ”€â”€ ğŸ“„ config.py                          # Configuration management
â”‚
â”œâ”€â”€ ğŸš€ run_pipeline.py                    # Main data processing pipeline
â”œâ”€â”€ ğŸ§ª test_chatbot.py                    # Interactive testing
â”‚
â”œâ”€â”€ ğŸ“ src/                               # Source code
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_processor.py                 # PDF extraction & cleaning
â”‚   â”œâ”€â”€ chunker.py                        # Semantic chunking
â”‚   â”œâ”€â”€ vector_store.py                   # ChromaDB vector database
â”‚   â”œâ”€â”€ rag_engine.py                     # Core RAG logic
â”‚   â””â”€â”€ api.py                            # FastAPI server
â”‚
â””â”€â”€ ğŸ“ data/                              # Data directory
    â”œâ”€â”€ ğŸ“ SOP and KB Docs/               # 93 PDF files (input)
    â”œâ”€â”€ ğŸ“ kb/                            # 10 markdown KB articles
    â”œâ”€â”€ ğŸ“ processed/                     # Processed data (output)
    â”‚   â”œâ”€â”€ all_documents_cleaned.json    # Cleaned documents
    â”‚   â”œâ”€â”€ final_chunks.json             # Semantic chunks
    â”‚   â”œâ”€â”€ processing_report.json        # Processing stats
    â”‚   â””â”€â”€ chunk_statistics.json         # Chunk stats
    â””â”€â”€ ğŸ“ chroma/                        # Vector database (output)
        â””â”€â”€ [ChromaDB files]
```

## ğŸ”„ Data Flow

```
PDFs (93 files)
    â†“
[data_processor.py] â†’ Extract & Clean
    â†“
all_documents_cleaned.json
    â†“
[chunker.py] â†’ Semantic Chunking
    â†“
final_chunks.json
    â†“
[vector_store.py] â†’ Generate Embeddings
    â†“
ChromaDB Vector Database
    â†“
[rag_engine.py] â†’ Query Processing
    â†“
[api.py] â†’ REST API
    â†“
Zoho SalesIQ Webhook
```

## ğŸ“¦ Key Components

### 1. Data Processing (`src/data_processor.py`)
- Extracts text from PDFs using multiple methods
- Cleans OCR errors and formatting
- Categorizes by topic
- Outputs: `all_documents_cleaned.json`

### 2. Semantic Chunking (`src/chunker.py`)
- Creates 500-char chunks with 50-char overlap
- Preserves semantic boundaries
- Maintains metadata
- Outputs: `final_chunks.json`

### 3. Vector Store (`src/vector_store.py`)
- Generates OpenAI embeddings
- Stores in ChromaDB
- Enables semantic search
- Outputs: `data/chroma/` directory

### 4. RAG Engine (`src/rag_engine.py`)
- Retrieves relevant context
- Generates responses with OpenAI
- Handles escalation logic
- Maintains conversation history

### 5. API Server (`src/api.py`)
- FastAPI REST endpoints
- Zoho webhook integration
- Session management
- Health checks and monitoring

## ğŸ¯ Entry Points

### For Development
```bash
# Process all data
python run_pipeline.py

# Test chatbot interactively
python test_chatbot.py

# Test with predefined cases
python test_chatbot.py auto

# Start API server
python src/api.py
```

### For Production
```bash
# Start API with gunicorn
gunicorn src.api:app -w 4 -k uvicorn.workers.UvicornWorker

# Or with uvicorn directly
uvicorn src.api:app --host 0.0.0.0 --port 8000
```

## ğŸ“Š Data Files

### Input Files
- `data/SOP and KB Docs/*.pdf` - 93 PDF documents
- `data/kb/*.md` - 10 markdown KB articles

### Output Files
- `data/processed/all_documents_cleaned.json` - ~93 documents
- `data/processed/final_chunks.json` - ~1000-1500 chunks
- `data/chroma/` - Vector database with embeddings

### Configuration Files
- `.env` - Environment variables (API keys, settings)
- `config.py` - Configuration management

## ğŸ”Œ API Endpoints

```
GET  /                    - Health check
GET  /health              - Detailed health status
POST /chat                - Main chat endpoint
POST /webhook/zoho        - Zoho SalesIQ webhook
POST /session/clear       - Clear conversation session
GET  /stats               - API statistics
```

## ğŸ§ª Testing

```bash
# Interactive testing
python test_chatbot.py

# Automated test suite
python test_chatbot.py auto

# API testing
curl http://localhost:8000/health
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{"query": "I forgot my password"}'
```

## ğŸ“ˆ Monitoring

### Logs
- Console output during development
- Production: Configure logging in `src/api.py`

### Metrics
- `/stats` endpoint - Active sessions, vector store stats
- `/health` endpoint - System health check
- OpenAI dashboard - Token usage and costs

## ğŸ” Security

### Environment Variables
- `OPENAI_API_KEY` - OpenAI API key (required)
- `API_SECRET_KEY` - API authentication (recommended)
- `ZOHO_WEBHOOK_SECRET` - Webhook signature verification

### Best Practices
- Never commit `.env` file
- Use webhook signature verification
- Implement rate limiting in production
- Monitor API usage and costs

## ğŸš€ Deployment Checklist

- [ ] Install dependencies: `pip install -r requirements.txt`
- [ ] Configure `.env` with API keys
- [ ] Run data pipeline: `python run_pipeline.py`
- [ ] Test chatbot: `python test_chatbot.py auto`
- [ ] Start API: `python src/api.py`
- [ ] Test API endpoints
- [ ] Deploy to production server
- [ ] Configure Zoho webhook
- [ ] Monitor performance
- [ ] Collect feedback and iterate

## ğŸ“ Support

For questions or issues:
1. Check README.md for documentation
2. Review IMPLEMENTATION_PLAN.md for detailed steps
3. Check API docs: http://localhost:8000/docs
4. Review logs and error messages
